{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Regression with Gaussian Processes\n",
    "\n",
    "------------------------------------------------------\n",
    "*Machine Learning, Master in Big Data Analytics, 2018-2019*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "The aim of this homework is to solve a real data problem using the Gaussian Process implementation of GPy. The documentation of GPy is avaialable from the [SheffieldML github page](https://github.com/SheffieldML/GPy) or from [this page](http://gpy.readthedocs.org/en/latest/). \n",
    "\n",
    "The problem is the prediction of both the heating load (HL) and cooling load (CL) of residential buildings. We consider eight input variables for each building: relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution.\n",
    "\n",
    "In this [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X) you can find a detailed description of the problem and a solution based on linear regression [(iteratively reweighted least squares (IRLS) algorithm)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&ved=2ahUKEwjZuoLY2OjgAhUs3uAKHUZ7BVcQFjAJegQIAhAC&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F9b92%2F18e7233f4d0b491e1582c893c9a099470a73.pdf&usg=AOvVaw3YDwqZh1xyF626VqfnCM2k) and random forests. Using GPs, our goal is not only estimate accurately both HL and CL, but also get a measure of uncertainty in our predictions.\n",
    "\n",
    "The data set can be downloaded from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preparing the data\n",
    "\n",
    "* Download the dataset\n",
    "* Divide at random the dataset into train (80%) and test (20%) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((614L, 8L), (614L, 2L))\n",
      "((154L, 8L), (154L, 2L))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enb = pd.read_excel('ENB2012_data.xlsx')\n",
    "enb = enb.values\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "X=enb[:,0:8]\n",
    "y=enb[:,8:10]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting and optimizing the model\n",
    "\n",
    "You will train two independent GPs, one to estimate HL and one to estimate CL. For each of the two GPs ...\n",
    "\n",
    "**On the training data set:**\n",
    "\n",
    "a) Build a GP regression model based on a RBF kernel with ARD, in which each input dimension is weighted with a different lengthscale. **1.5 points**\n",
    "\n",
    "b) Fit the covariance function parameters and noise variance. **1 point** \n",
    "\n",
    "c) According to the ARD parameters found, what variables are more important for the regression? Compare it to Table 8 in this [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X) **1.5 points**\n",
    "\n",
    "**On the test data set:**\n",
    "\n",
    "d) Compute the test mean absolute error error and the test mean square error (MSE)  using the GP posterior mean and the optimized hyperparameters. Compare your results with Tables 6 and 7 in this [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X).\n",
    "**1.5 points**\n",
    "\n",
    "2) Try to improve your results by using a more complicated kernel, in which you combine various covariance functions. In this [link](http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/basic_kernels.ipynb) you can see how to define different kernels and combine  them. Comments the results. **2 points**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A & B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'   ##QUALITY FIGURES!!\n",
    "plt.rcParams[\"figure.figsize\"] = [8,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel\n",
    "ker = GPy.kern.RBF(input_dim=8, ARD=True) \n",
    "\n",
    "# create simple GP model for each variable\n",
    "m = GPy.models.GPRegression(X_train,y_train,ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".pd{\n",
       "    font-family: \"Courier New\", Courier, monospace !important;\n",
       "    width: 100%;\n",
       "    padding: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<p class=pd>\n",
       "<b>Model</b>: GP regression<br>\n",
       "<b>Objective</b>: 42053.891863<br>\n",
       "<b>Number of Parameters</b>: 10<br>\n",
       "<b>Number of Optimization Parameters</b>: 10<br>\n",
       "<b>Updates</b>: True<br>\n",
       "</p>\n",
       "<style type=\"text/css\">\n",
       ".tg  {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\"><tr><th><b>  GP_regression.         </b></th><th><b>value</b></th><th><b>constraints</b></th><th><b>priors</b></th></tr>\n",
       "<tr><td class=tg-left>  rbf.variance           </td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  rbf.lengthscale        </td><td class=tg-right>(8L,)</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  Gaussian_noise.variance</td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<GPy.models.gp_regression.GPRegression at 0xd8af978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720cd181d28d49c294c61891649df117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEJveChjaGlsZHJlbj0oVkJveChjaGlsZHJlbj0oSW50UHJvZ3Jlc3ModmFsdWU9MCwgbWF4PTEwMDApLCBIVE1MKHZhbHVlPXUnJykpKSwgQm94KGNoaWxkcmVuPShIVE1MKHZhbHVlPXUnJynigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<paramz.optimization.optimization.opt_lbfgsb at 0x52289b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.optimize(messages=True, max_f_eval = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel Lengthscale:\n",
      "\n",
      "[  1.           1.           1.           1.           1.\n",
      " 234.14905259   0.32181893 264.10068573]\n"
     ]
    }
   ],
   "source": [
    "print(\"kernel Lengthscale:\")\n",
    "print(\"\")\n",
    "print(ker.lengthscale.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "\n",
    "#Generate predictions from the model\n",
    "posteriortest = m.posterior_samples_f(X_test, full_cov=True, size=nsamples)\n",
    "meantest = m.predict(X_test, full_cov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 0.34413247326073054\n",
      "Y1 Mean Squared Error: 0.222068776142243\n",
      "Y2 Mean Absolute Error: 0.9339193544970908\n",
      "Y2 Mean Squared Error: 2.112545075277692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meantest[0][:,0])))\n",
    "print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meantest[0][:,0])))\n",
    "\n",
    "print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meantest[0][:,1])))\n",
    "print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meantest[0][:,1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 0.3183214614892159\n",
      "Y1 Mean Squared Error: 0.2636062882688694\n",
      "Y2 Mean Absolute Error: 0.41294483496577733\n",
      "Y2 Mean Squared Error: 0.3285022890175988\n"
     ]
    }
   ],
   "source": [
    "ker_combined = GPy.kern.RBF(input_dim=8, ARD=True) + GPy.kern.White(8) + GPy.kern.Matern32(8)\n",
    "\n",
    "# create simple GP model for each variable\n",
    "m_improved = GPy.models.GPRegression(X_train,y_train,ker_combined)\n",
    "\n",
    "m_improved.optimize(messages=False, max_f_eval = 1000)\n",
    "\n",
    "meanImprovedtest = m_improved.predict(X_test, full_cov=False)\n",
    "\n",
    "print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "\n",
    "print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meanImprovedtest[0][:,1])))\n",
    "print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meanImprovedtest[0][:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 20.982272727272726\n",
      "Y1 Mean Squared Error: 526.4627837662338\n",
      "Y2 Mean Absolute Error: 23.318376623376622\n",
      "Y2 Mean Squared Error: 616.558175974026\n"
     ]
    }
   ],
   "source": [
    "ker_combined = GPy.kern.RBF(input_dim=8, ARD=True) * GPy.kern.White(8) * GPy.kern.Matern32(8)\n",
    "\n",
    "# create simple GP model for each variable\n",
    "m_improved = GPy.models.GPRegression(X_train,y_train,ker_combined)\n",
    "\n",
    "m_improved.optimize(messages=False, max_f_eval = 1000)\n",
    "\n",
    "meanImprovedtest = m_improved.predict(X_test, full_cov=False)\n",
    "\n",
    "\n",
    "print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "\n",
    "print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meanImprovedtest[0][:,1])))\n",
    "print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meanImprovedtest[0][:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:137: RuntimeWarning:overflow encountered in square\n",
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:168: RuntimeWarning:overflow encountered in divide\n",
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:454: RuntimeWarning:overflow encountered in multiply\n",
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:454: RuntimeWarning:overflow encountered in exp\n",
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:454: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 0.3479557572364571\n",
      "Y1 Mean Squared Error: 0.3086263190704844\n",
      "Y2 Mean Absolute Error: 0.4406707757816722\n",
      "Y2 Mean Squared Error: 0.36590540886003614\n"
     ]
    }
   ],
   "source": [
    "ker_combined = GPy.kern.RBF(input_dim=8, ARD=True) + GPy.kern.White(8) + GPy.kern.Matern32(8) + GPy.kern.Exponential(8)\n",
    "\n",
    "# create simple GP model for each variable\n",
    "m_improved = GPy.models.GPRegression(X_train,y_train,ker_combined)\n",
    "\n",
    "m_improved.optimize(messages=False, max_f_eval = 1000)\n",
    "\n",
    "meanImprovedtest = m_improved.predict(X_test, full_cov=False)\n",
    "\n",
    "print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "\n",
    "print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meanImprovedtest[0][:,1])))\n",
    "print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meanImprovedtest[0][:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker_combined = GPy.kern.RBF(input_dim=8, ARD=True) + GPy.kern.White(8) + GPy.kern.Exponential(8)\n",
    "\n",
    "# create simple GP model for each variable\n",
    "m_improved = GPy.models.GPRegression(X_train,y_train,ker_combined)\n",
    "\n",
    "m_improved.optimize(messages=False, max_f_eval = 1000)\n",
    "\n",
    "meanImprovedtest = m_improved.predict(X_test, full_cov=False)\n",
    "\n",
    "print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meanImprovedtest[0][:,0])))\n",
    "\n",
    "print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meanImprovedtest[0][:,1])))\n",
    "print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meanImprovedtest[0][:,1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse GP implementation \n",
    "\n",
    "Try to implement an sparse version of the GP regressor, optimized to find a set of **inducing points** that the GP relies on to do the prediction. Measure the test error prediction for 20, 40, and 100 inducing points. **2.5 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 5.816672544200181\n",
      "Y1 Mean Squared Error: 93.17450616112286\n",
      "Y2 Mean Absolute Error: 5.9682847154536\n",
      "Y2 Mean Squared Error: 113.6507607582618\n",
      "\n",
      "Y1 Mean Absolute Error: 1.4400630714376288\n",
      "Y1 Mean Squared Error: 5.104248694700504\n",
      "Y2 Mean Absolute Error: 1.4300563904781334\n",
      "Y2 Mean Squared Error: 4.216733578155614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\paramz\\transformations.py:111: RuntimeWarning:overflow encountered in expm1\n",
      " C:\\Users\\niall\\Anaconda2\\lib\\site-packages\\paramz\\transformations.py:111: RuntimeWarning:invalid value encountered in expm1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1 Mean Absolute Error: 0.6693802153271133\n",
      "Y1 Mean Squared Error: 0.8179704748808401\n",
      "Y2 Mean Absolute Error: 1.0054031055057004\n",
      "Y2 Mean Squared Error: 2.2771548101861567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = [20,40,100]\n",
    "\n",
    "for i in search_space:\n",
    "    m_sp = GPy.models.SparseGPRegression(X_train, y_train, num_inducing=i, kernel=ker)\n",
    "    m_sp.optimize(messages=False, max_f_eval = 1000)\n",
    "    meantest_sp = m_sp.predict(X_test, full_cov=False)\n",
    "\n",
    "    print(\"Y1 Mean Absolute Error: \" + str(mean_absolute_error(y1_test, meantest_sp[0][:,0])))\n",
    "    print(\"Y1 Mean Squared Error: \" + str(mean_squared_error(y1_test, meantest_sp[0][:,0])))\n",
    "\n",
    "    print(\"Y2 Mean Absolute Error: \" + str(mean_absolute_error(y2_test, meantest_sp[0][:,1])))\n",
    "    print(\"Y2 Mean Squared Error: \" + str(mean_squared_error(y2_test, meantest_sp[0][:,1])))\n",
    "    \n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
