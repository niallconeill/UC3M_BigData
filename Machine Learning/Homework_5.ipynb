{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Topic Modelling with Latent Dirichlet Allocation\n",
    "\n",
    "------------------------------------------------------\n",
    "*Machine Learning, Master in Big Data Analytics, 2018-2019*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "The goal of this homework is to first introduce the pre-processing tasks that one has to run over a corpus of documents before analyzing its structure with topic models. We will use the library [nltk](https://www.nltk.org/) to perform document tokenization, stemming, and lemmatization. Then, you will explore the library [gensim](https://radimrehurek.com/gensim/) and implement LDA over the processed database.\n",
    "\n",
    "\n",
    "The data set we’ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the CSV and save it to 'data_text'\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "\n",
    "# We only need the Headlines text column from the data. We reduce the database size to 3000000 headlines, \n",
    "# so model training does not take that long\n",
    "\n",
    "data_text = data[:300000][['headline_text']];\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>australia to contribute 10 million in aid to iraq</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>barca take record as robson celebrates birthda...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bathhouse plans move ahead</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big hopes for launceston cycling championship</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>big plan to boost paroo water supplies</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blizzard buries united states in bills</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>brigadier dismisses reports troops harassed in</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>british combat troops arriving daily in kuwait</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bryant leads lakers to double overtime win</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bushfire victims urged to see centrelink</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>businesses should prepare for terrorist attacks</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>calleri avenges final defeat to eliminate massu</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>call for ethanol blend fuel to go ahead</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>carews freak goal leaves roma in ruins</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cemeteries miss out on funds</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>code of conduct toughens organ donation regula...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>commonwealth bank cuts fixed home loan rates</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>community urged to help homeless youth</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>council chief executive fails to secure position</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>councillor to contest wollongong as independent</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>council moves to protect tas heritage garden</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>council welcomes ambulance levy decision</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>council welcomes insurance breakthrough</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>crean tells alp leadership critics to shut up</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dargo fire threat expected to rise</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>death toll continues to climb in s korean subway</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dems hold plebiscite over iraqi conflict</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dent downs philippoussis in tie break thriller</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>de villiers to learn fate on march 5</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>digital tv will become commonplace summit</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>direct anger at govt not soldiers crean urges</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>dispute over at smithton vegetable processing ...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>dog mauls 18 month old toddler in nsw</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>dying korean subway passengers phoned for help</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>england change three for wales match</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>epa still trying to recover chemical clean up ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>expressions of interest sought to build livestock</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>fed opp to re introduce national insurance</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>firefighters contain acid spill</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>four injured in head on highway crash</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        headline_text  index\n",
       "0   aba decides against community broadcasting lic...      0\n",
       "1      act fire witnesses must be aware of defamation      1\n",
       "2      a g calls for infrastructure protection summit      2\n",
       "3            air nz staff in aust strike for pay rise      3\n",
       "4       air nz strike to affect australian travellers      4\n",
       "5                   ambitious olsson wins triple jump      5\n",
       "6          antic delighted with record breaking barca      6\n",
       "7   aussie qualifier stosur wastes four memphis match      7\n",
       "8        aust addresses un security council over iraq      8\n",
       "9          australia is locked into war timetable opp      9\n",
       "10  australia to contribute 10 million in aid to iraq     10\n",
       "11  barca take record as robson celebrates birthda...     11\n",
       "12                         bathhouse plans move ahead     12\n",
       "13      big hopes for launceston cycling championship     13\n",
       "14             big plan to boost paroo water supplies     14\n",
       "15             blizzard buries united states in bills     15\n",
       "16     brigadier dismisses reports troops harassed in     16\n",
       "17     british combat troops arriving daily in kuwait     17\n",
       "18         bryant leads lakers to double overtime win     18\n",
       "19           bushfire victims urged to see centrelink     19\n",
       "20    businesses should prepare for terrorist attacks     20\n",
       "21    calleri avenges final defeat to eliminate massu     21\n",
       "22            call for ethanol blend fuel to go ahead     22\n",
       "23             carews freak goal leaves roma in ruins     23\n",
       "24                       cemeteries miss out on funds     24\n",
       "25  code of conduct toughens organ donation regula...     25\n",
       "26       commonwealth bank cuts fixed home loan rates     26\n",
       "27             community urged to help homeless youth     27\n",
       "28   council chief executive fails to secure position     28\n",
       "29    councillor to contest wollongong as independent     29\n",
       "30       council moves to protect tas heritage garden     30\n",
       "31           council welcomes ambulance levy decision     31\n",
       "32            council welcomes insurance breakthrough     32\n",
       "33      crean tells alp leadership critics to shut up     33\n",
       "34                 dargo fire threat expected to rise     34\n",
       "35   death toll continues to climb in s korean subway     35\n",
       "36           dems hold plebiscite over iraqi conflict     36\n",
       "37     dent downs philippoussis in tie break thriller     37\n",
       "38               de villiers to learn fate on march 5     38\n",
       "39          digital tv will become commonplace summit     39\n",
       "40      direct anger at govt not soldiers crean urges     40\n",
       "41  dispute over at smithton vegetable processing ...     41\n",
       "42              dog mauls 18 month old toddler in nsw     42\n",
       "43     dying korean subway passengers phoned for help     43\n",
       "44               england change three for wales match     44\n",
       "45  epa still trying to recover chemical clean up ...     45\n",
       "46  expressions of interest sought to build livestock     46\n",
       "47         fed opp to re introduce national insurance     47\n",
       "48                    firefighters contain acid spill     48\n",
       "49              four injured in head on highway crash     49"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview documents\n",
    "\n",
    "documents.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data pre-processing\n",
    "We will perform the following steps for data pre-processing:\n",
    "\n",
    "1. Tokenization.\n",
    "2. Lowercase the words and remove punctuation.\n",
    "3. Remove stop words.\n",
    "4. Stemming and Lemmatization.\n",
    "\n",
    "**Tokenization** is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens.\n",
    "\n",
    "**“Stop words** are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.\n",
    "\n",
    "**Stemming** is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look).\n",
    "\n",
    "**The aim of lemmatization**, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\niall\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer example\n",
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v')) # past tense to present tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemmer example\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing steps on the entire dataset\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "# Preview a document after preprocessing\n",
    "\n",
    "document_num = 4310\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "# Bag of words on the dataset\n",
    "# Create a dictionary from 'processed_docs'\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Checking dictionary created\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove words that haven been seldom used\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a bag-of-words document respresentation\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75, 1), (76, 1), (77, 1), (78, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_num = 20\n",
    "\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 75 (\"attack\") appears 1 time.\n",
      "Word 76 (\"busi\") appears 1 time.\n",
      "Word 77 (\"prepar\") appears 1 time.\n",
      "Word 78 (\"terrorist\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview BOW for our sample preprocessed document\n",
    "\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (4 points):** Run LDA with 10 topics over the processed corpus of headlines using the the LDA implementation in the [gensim](https://radimrehurek.com/gensim/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus=bow_corpus, id2word = dictionary, num_topics = 11, update_every = 1, passes = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (2 points):** Print the 10 most probable words in every topic. Can you interpret the meaning of each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  [('water', 0.061926607),\n",
       "   ('plan', 0.039452564),\n",
       "   ('group', 0.020389773),\n",
       "   ('govt', 0.020238133),\n",
       "   ('council', 0.017405178),\n",
       "   ('attack', 0.017086718),\n",
       "   ('green', 0.017064193),\n",
       "   ('home', 0.016446479),\n",
       "   ('time', 0.012299835),\n",
       "   ('question', 0.011681642)]),\n",
       " (9,\n",
       "  [('polic', 0.06812261),\n",
       "   ('urg', 0.03079354),\n",
       "   ('help', 0.023746645),\n",
       "   ('closer', 0.02069798),\n",
       "   ('school', 0.017897306),\n",
       "   ('resid', 0.01558193),\n",
       "   ('driver', 0.014870643),\n",
       "   ('australian', 0.014323024),\n",
       "   ('probe', 0.013226297),\n",
       "   ('arrest', 0.013063837)]),\n",
       " (5,\n",
       "  [('charg', 0.04166429),\n",
       "   ('court', 0.035764642),\n",
       "   ('face', 0.033504464),\n",
       "   ('accus', 0.021950422),\n",
       "   ('murder', 0.021135312),\n",
       "   ('case', 0.015734756),\n",
       "   ('indigen', 0.015662214),\n",
       "   ('road', 0.015103767),\n",
       "   ('fight', 0.014883258),\n",
       "   ('jail', 0.014655143)]),\n",
       " (7,\n",
       "  [('elect', 0.024042891),\n",
       "   ('offer', 0.019546736),\n",
       "   ('fear', 0.0193431),\n",
       "   ('centr', 0.018985467),\n",
       "   ('deal', 0.0187054),\n",
       "   ('want', 0.018188717),\n",
       "   ('play', 0.016532505),\n",
       "   ('launch', 0.01367543),\n",
       "   ('clash', 0.012415094),\n",
       "   ('port', 0.01204196)]),\n",
       " (8,\n",
       "  [('kill', 0.041591633),\n",
       "   ('push', 0.02820123),\n",
       "   ('hous', 0.026571797),\n",
       "   ('sydney', 0.026179835),\n",
       "   ('worker', 0.021939337),\n",
       "   ('blaze', 0.018076979),\n",
       "   ('minist', 0.015946092),\n",
       "   ('need', 0.015340306),\n",
       "   ('decis', 0.014599815),\n",
       "   ('chief', 0.014505337)]),\n",
       " (1,\n",
       "  [('power', 0.025921406),\n",
       "   ('test', 0.023991179),\n",
       "   ('record', 0.02148832),\n",
       "   ('mayor', 0.019276336),\n",
       "   ('secur', 0.019200891),\n",
       "   ('teen', 0.017914653),\n",
       "   ('england', 0.017578116),\n",
       "   ('melbourn', 0.01592465),\n",
       "   ('river', 0.013468583),\n",
       "   ('hop', 0.013286612)]),\n",
       " (4,\n",
       "  [('crash', 0.042097304),\n",
       "   ('iraq', 0.028544895),\n",
       "   ('opposit', 0.023787778),\n",
       "   ('hospit', 0.023733057),\n",
       "   ('deni', 0.020078393),\n",
       "   ('die', 0.018283226),\n",
       "   ('howard', 0.017951129),\n",
       "   ('investig', 0.016288202),\n",
       "   ('win', 0.016247354),\n",
       "   ('troop', 0.016209574)]),\n",
       " (6,\n",
       "  [('warn', 0.030346025),\n",
       "   ('reject', 0.0249071),\n",
       "   ('claim', 0.023717819),\n",
       "   ('chang', 0.020640261),\n",
       "   ('talk', 0.017349252),\n",
       "   ('take', 0.016454982),\n",
       "   ('lead', 0.013552524),\n",
       "   ('expect', 0.012429526),\n",
       "   ('blame', 0.011990004),\n",
       "   ('issu', 0.011247607)]),\n",
       " (0,\n",
       "  [('back', 0.022184657),\n",
       "   ('year', 0.022073634),\n",
       "   ('world', 0.020445054),\n",
       "   ('price', 0.01874484),\n",
       "   ('high', 0.018342944),\n",
       "   ('market', 0.016483003),\n",
       "   ('appeal', 0.015039155),\n",
       "   ('nation', 0.014632307),\n",
       "   ('storm', 0.013434483),\n",
       "   ('aussi', 0.0127670495)]),\n",
       " (2,\n",
       "  [('death', 0.033310175),\n",
       "   ('miss', 0.027530385),\n",
       "   ('aust', 0.026756117),\n",
       "   ('polic', 0.021428538),\n",
       "   ('search', 0.018752713),\n",
       "   ('continu', 0.017553855),\n",
       "   ('rise', 0.015112886),\n",
       "   ('hick', 0.014983431),\n",
       "   ('report', 0.013995206),\n",
       "   ('begin', 0.013121631)])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "lda.show_topics(formatted =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (2 points):**\n",
    "\n",
    "Print the proportion of topics in document number 17320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.022727275),\n",
       " (1, 0.27278066),\n",
       " (2, 0.022727393),\n",
       " (3, 0.022730427),\n",
       " (4, 0.022727273),\n",
       " (5, 0.022727704),\n",
       " (6, 0.022727974),\n",
       " (7, 0.022727916),\n",
       " (8, 0.02272795),\n",
       " (9, 0.022727273),\n",
       " (10, 0.5226682)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = bow_corpus[17320]\n",
    "\n",
    "lda.get_document_topics(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (2 points):**\n",
    "\n",
    "Find the 10 most similar documents to document number 17320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity \n",
    "\n",
    "index = MatrixSimilarity(bow_corpus, num_features = len(dictionary), num_best = 10)\n",
    "\n",
    "similar_docs = index[vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17320, 0.9999999403953552), (218668, 0.6666666269302368), (71926, 0.6666666269302368), (89070, 0.5773502588272095), (160198, 0.5773502588272095), (80394, 0.5773502588272095), (19351, 0.5773502588272095), (222894, 0.5773502588272095), (100011, 0.5773502588272095), (258039, 0.5773502588272095)]\n"
     ]
    }
   ],
   "source": [
    "print(similar_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
