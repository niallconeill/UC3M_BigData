{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">&lt;pyspark.sql.session.SparkSession object at 0x7f3dd7319588&gt;\n",
       "&lt;SparkContext master=local[8] appName=Databricks Shell&gt;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "##########################################################################################################\n",
    "# SPARK CONTEXT INITIALIZATION\n",
    "# Something like this is required if you want to use SPARK in Windows. In linux/ubuntu would be similar\n",
    "SPARK_HOME = \"\"\"C:/Users/Karolina/Downloads/spark-2.4.0-bin-hadoop2.7\"\"\" #CHANGE THIS PATH TO YOURS!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.7-src.zip\")) #BEWARE WITH py4j version!!\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"pyspark.zip\"))\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "##########################################################################################################\n",
    "\n",
    "# This is the spark context\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wind_sd = spark.read.csv(path=\"/FileStore/tables/wind.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First remove the attributes that cannot be used for prediction\n",
    "# For now, we will keep 'year' here, as it will need to be used for splitting the data into train, validation and test sets\n",
    "wind_sd = wind_sd.drop('steps')\n",
    "wind_sd = wind_sd.drop('month')\n",
    "wind_sd = wind_sd.drop('day')\n",
    "wind_sd = wind_sd.drop('hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then only select attributes containing 'u100', 'v100', 'u10', 'v10', 'ienss' and 'iews'\n",
    "# 'energy' is kept as it is response, 'year' kept as explained above\n",
    "selected = []\n",
    "for col in wind_sd.columns:\n",
    "    if(('energy' in col) or('year' in col) or ('u100' in col) or ('v100' in col) or ('u10' in col) or ('v10' in col) or ('ienss' in col) or ('iews' in col)):\n",
    "    selected.append(col)\n",
    "\n",
    "wind_sd = wind_sd.select(*selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now split the data into training, validation and test sets\n",
    "trainingData_sd = wind_sd.filter(\"2005 <= year and year <= 2006\")\n",
    "validationData_sd = wind_sd.filter(\"2007 <= year and year <= 2008\")\n",
    "testData_sd = wind_sd.filter(\"2009 <= year and year <= 2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then remove the 'year' column from all datasets:\n",
    "trainingData_sd = trainingData_sd.drop('year')\n",
    "validationData_sd = validationData_sd.drop('year')\n",
    "testData_sd = testData_sd.drop('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the dataframes for ML use\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# First rename 'energy' variable to 'label' in order to be compatible with spark ML libraries\n",
    "trainingData_sd = trainingData_sd.withColumnRenamed(\"energy\", \"label\")\n",
    "validationData_sd = validationData_sd.withColumnRenamed(\"energy\", \"label\")\n",
    "testData_sd = testData_sd.withColumnRenamed(\"energy\", \"label\")\n",
    "\n",
    "ignore = ['label']\n",
    "\n",
    "# First for training data frame\n",
    "training_assembler = VectorAssembler(\n",
    "    inputCols=[x for x in trainingData_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingData_sd = training_assembler.transform(trainingData_sd).select(['label', 'features'])\n",
    "\n",
    "# Then for validation data frame\n",
    "validation_assembler = VectorAssembler(\n",
    "    inputCols=[x for x in validationData_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "validationData_sd = validation_assembler.transform(validationData_sd).select(['label', 'features'])\n",
    "\n",
    "# Lastly for testing data frame\n",
    "test_assembler = VectorAssembler(\n",
    "    inputCols=[x for x in testData_sd.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "testData_sd = test_assembler.transform(testData_sd).select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">MAE for k=0 is 557.1766578538993\n",
       "MAE for k=1 is 423.8363800642306\n",
       "MAE for k=2 is 376.54308510289553\n",
       "MAE for k=3 is 339.8868961531615\n",
       "MAE for k=4 is 329.7516337602435\n",
       "MAE for k=5 is 306.9945513022891\n",
       "MAE for k=6 is 300.43417479304003\n",
       "MAE for k=7 is 305.01983377795284\n",
       "MAE for k=8 is 305.9652274273762\n",
       "MAE for k=9 is 310.71194911488243\n",
       "MAE for k=10 is 323.5853861445507\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define regression evaluator that will be used to compare predictions using the value of MAE for different models\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# Train and test multiple decision trees to find the optimal max depth\n",
    "for i in range(0, 11):\n",
    "    dt = DecisionTreeRegressor(maxDepth=i)\n",
    "    dt = dt.fit(trainingData_sd)\n",
    "    predictors_dt = dt.transform(validationData_sd)\n",
    "    mae_dt = evaluator.evaluate(predictors_dt)\n",
    "    print(\"MAE for k={} is {}\".format(i, mae_dt))\n",
    "    # In this case the optimal maximum depth is 6 as it returns the lowest MAE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">MAE with optimal depth = 6 is: 322.35278957558893\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine training data and validation data into one set\n",
    "combined_sd = trainingData_sd.union(validationData_sd)\n",
    "\n",
    "# Train the model with train and validation data using optimal depth = 6\n",
    "dt = DecisionTreeRegressor(maxDepth=6)\n",
    "dt = dt.fit(combined_sd)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictors_sd = dt.transform(testData_sd)\n",
    "mae = evaluator.evaluate(predictors_sd)\n",
    "print(\"MAE with optimal depth = 6 is: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">MAE for k=2 is: 376.54308510289553\n",
       "MAE for k=3 is: 339.8868961531615\n",
       "MAE for k=4 is: 329.7516337602435\n",
       "MAE for k=5 is: 306.9945513022891\n",
       "MAE for k=6 is: 300.43417479304003\n",
       "MAE for k=7 is: 305.01983377795284\n",
       "MAE for k=8 is: 305.9652274273762\n",
       "MAE for k=9 is: 310.71194911488243\n",
       "MAE for k=10 is: 323.5853861445507\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we try the original way of calculating optimal number of PCAs in a loop, evaluating cases one by one subsequently, however, this way of doing things is not optimal since it includes redundant recalculation of PCAs in every loop\n",
    "for i in range(2,11):\n",
    "    pca0 = PCA(k=i, inputCol=\"features\")\n",
    "    dt0 = DecisionTreeRegressor(maxDepth=i)\n",
    "    ppl0 = Pipeline(stages=[pca0, dt0])\n",
    "    model0 = ppl0.fit(trainingData_sd)\n",
    "    predictions0 = model0.transform(validationData_sd)\n",
    "    mae0 = evaluator.evaluate(predictions0)\n",
    "    print(\"MAE for k={} is: {}\".format(i, mae0))\n",
    "  # This returns the optimal k=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "# The method for finding the optimal number of PCAs without recalculating them every time, is by calculating the variance explained by the first k components\n",
    "\n",
    "# This has been done using the main ideas found here: https://stackoverflow.com/questions/33428589/pyspark-and-pca-how-can-i-extract-the-eigenvectors-of-this-pca-how-can-i-calcu/33481471\n",
    "\n",
    "# Compute the covariance matrix for a given data frame\n",
    "def estimateCovariance(df):\n",
    "    m = df.select(df['features']).rdd.map(lambda x: x[0]).mean()\n",
    "    dfZeroMean = df.select(df['features']).rdd.map(lambda x:   x[0]).map(lambda x: x-m)\n",
    "    return dfZeroMean.map(lambda x: np.outer(x,x)).sum()/df.count()\n",
    "\n",
    "# Compute the top k PCAs and corresponding eigenvalues\n",
    "def pca(df, k=2):\n",
    "    cov = estimateCovariance(df)\n",
    "    col = cov.shape[1]\n",
    "    eigVals, eigVecs = eigh(cov)\n",
    "    inds = np.argsort(eigVals)\n",
    "    eigVecs = eigVecs.T[inds[-1:-(col+1):-1]]  \n",
    "    components = eigVecs[0:k]\n",
    "    eigVals = eigVals[inds[-1:-(col+1):-1]]  # sort eigenvals\n",
    "    return components.T, eigVals\n",
    "\n",
    "# Calculate the fraction of variance explained by the top k eigenvectors\n",
    "def varianceExplained(eigenvalues, k=1):\n",
    "    for i in range(1, k+1):\n",
    "        print(\"Variance explained for k={} is {}\".format(i, np.sum(eigenvalues[0:i])/np.sum(eigenvalues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Variance explained for k=1 is 0.7901474242143935\n",
       "Variance explained for k=2 is 0.990816249726345\n",
       "Variance explained for k=3 is 0.9955966441532622\n",
       "Variance explained for k=4 is 0.9976527498638323\n",
       "Variance explained for k=5 is 0.9983536867406788\n",
       "Variance explained for k=6 is 0.9988122836877619\n",
       "Variance explained for k=7 is 0.999206447155893\n",
       "Variance explained for k=8 is 0.9995212268114962\n",
       "Variance explained for k=9 is 0.9997238867991572\n",
       "Variance explained for k=10 is 0.9998545767852098\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As previously, test upto a maximum of 10 PCAs\n",
    "k = 10\n",
    "components, eigenvalues = pca(trainingData_sd, k)\n",
    "varianceExplained(eigenvalues, k)\n",
    "# 3 PCAs are enough to explain more than 99% of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">MAE using only 3 PCAs: 339.8010175140096\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Using k=3 as from the method regarding variance explained\n",
    "final_pca = PCA(k=3, inputCol=\"features\")\n",
    "final_dt = DecisionTreeRegressor(maxDepth=6,\n",
    "                                 featuresCol=final_pca.getOutputCol(),\n",
    "                                 labelCol=\"label\")\n",
    "\n",
    "ppl = Pipeline(stages=[final_pca, final_dt])\n",
    "\n",
    "final_model = ppl.fit(combined_sd)\n",
    "\n",
    "final_predictions = final_model.transform(testData_sd)\n",
    "\n",
    "final_mae = evaluator.evaluate(final_predictions)\n",
    "\n",
    "print(\"MAE using only 3 PCAs: {}\".format(final_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">MAE using only 6 PCAs: 348.62088840710817\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using k=6 as from the loop\n",
    "final_pca = PCA(k=6, inputCol=\"features\")\n",
    "final_dt = DecisionTreeRegressor(maxDepth=6,\n",
    "                                 featuresCol=final_pca.getOutputCol(),\n",
    "                                 labelCol=\"label\")\n",
    "\n",
    "ppl = Pipeline(stages=[final_pca, final_dt])\n",
    "\n",
    "final_model = ppl.fit(combined_sd)\n",
    "\n",
    "final_predictions = final_model.transform(testData_sd)\n",
    "\n",
    "final_mae = evaluator.evaluate(final_predictions)\n",
    "\n",
    "print(\"MAE using only 6 PCAs: {}\".format(final_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the end, it appears that using 3 PCAs gives better results than with 6 PCAs, and also, the MAE score is not that much higher than using all PCAs - 322 compared with only using 3 - 339, but this is reducing the complexity of the model from 175 PCAs to only using 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Assignment 4",
  "notebookId": 1727042021411753
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
